= MultiIssuerJWTTokenAuthenticator End-to-End Testing
:toc:
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

link:../Specification.adoc[Back to Main Specification]

== End-to-End Testing Overview
_See Requirement link:../Requirements.adoc#NIFI-AUTH-16[NIFI-AUTH-16: Testing]_

This document outlines the end-to-end testing strategy for the MultiIssuerJWTTokenAuthenticator, focusing on automated UI testing with Cypress. The tests verify user flows through the application's user interface, ensuring that the entire application works as expected from a user's perspective with all UI components working together in a real environment.

[NOTE]
====
End-to-end testing typically takes 15-30 minutes for a complete test suite run, depending on the environment specifications and test coverage.
====

== Testing Goals and Objectives

=== Primary Goals

1. **Verify Complete User Flows**: Test entire user journeys from start to finish
2. **Validate UI Functionality**: Ensure all UI components work correctly
3. **Test Integration Points**: Verify proper integration between UI and backend services
4. **Ensure Data Integrity**: Validate that data flows correctly through the system
5. **Detect Regression Issues**: Identify when changes break existing functionality

=== Success Criteria

1. All critical user flows pass automated end-to-end tests
2. Tests run reliably with minimal flakiness
3. Test coverage includes all major UI components and interactions
4. Tests are maintainable and can be extended as new features are added
5. Tests are integrated into the CI/CD pipeline

== Testing Tools and Framework

=== Core Technologies

The end-to-end testing infrastructure uses modern, industry-standard tools:

* **Cypress** (v12.x or higher): Framework for end-to-end testing and cross-browser testing
* **Docker** (v24.x or higher): For containerized testing environment
* **Integration Testing Module**: For controlling runtime instances including NiFi and Keycloak
* **cui-test-keycloak-integration** (v1.0.x): For integration testing with Keycloak
* **CI/CD Integration**: GitHub Actions for automated test execution

[TIP]
====
For definitions of specialized testing terms used in this document:

* **Flakiness**: Tests that pass or fail inconsistently when no changes are made to the code
* **NAR file**: NiFi Archive file, a package format for NiFi processors
* **JWKS**: JSON Web Key Set, a standard format for publishing public keys used to verify JWT signatures
====

=== Integration Testing Module

The project includes a dedicated `integration-testing` module that provides a Docker-based test environment with:

* **NiFi Instance**: Running the latest version with the MultiIssuerJWTTokenAuthenticator processor
  * Resource requirements: 2 CPU cores, 2GB RAM minimum
  * Exposed on HTTPS port 9095
* **Keycloak Server**: For generating valid JWT tokens and simulating different identity providers
  * Resource requirements: 1 CPU core, 1GB RAM minimum
  * Exposed on HTTP port 9080 and HTTPS port 9085
* **Pre-configured Certificates**: For secure communication between components
* **Helper Scripts**: For starting, stopping, and managing the environment

This module simplifies end-to-end testing by providing a consistent, reproducible environment that closely mirrors production deployments.

=== Maven Integration

End-to-end tests are integrated into the Maven build process using the `frontend-maven-plugin` for UI tests and the `maven-failsafe-plugin` for integration tests:

[source,xml]
----
<!-- In parent pom.xml, these properties are defined: -->
<!-- <version.frontend-maven-plugin>1.12.1</version.frontend-maven-plugin> -->
<!-- <version.nodejs>16.17.0</version.nodejs> -->

<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-failsafe-plugin</artifactId>
    <executions>
        <execution>
            <goals>
                <goal>integration-test</goal>
                <goal>verify</goal>
            </goals>
            <configuration>
                <systemPropertyVariables>
                    <nifi.url>https://localhost:9095/nifi/</nifi.url>
                    <keycloak.url>http://localhost:9080/</keycloak.url>
                    <keycloak.secure.url>https://localhost:9085/</keycloak.secure.url>
                </systemPropertyVariables>
            </configuration>
        </execution>
    </executions>
</plugin>
----

For UI testing, the `frontend-maven-plugin` is used:

[source,xml]
----
<plugin>
    <groupId>com.github.eirslett</groupId>
    <artifactId>frontend-maven-plugin</artifactId>
    <version>${version.frontend-maven-plugin}</version>
    <configuration>
        <nodeVersion>${version.nodejs}</nodeVersion>
        <installDirectory>target</installDirectory>
    </configuration>
    <executions>
        <execution>
            <id>cypress-run</id>
            <goals>
                <goal>npm</goal>
            </goals>
            <phase>integration-test</phase>
            <configuration>
                <arguments>run e2e:test</arguments>
            </configuration>
        </execution>
    </executions>
</plugin>
----

== Test Environment Setup

The following diagram illustrates the architecture of the end-to-end testing environment. This visual representation helps understand the relationships between components and how they interact during testing:

image::../plantuml/test-environment-architecture.png[Test Environment Architecture, align="center"]

[NOTE]
====
The environment requires network connectivity between all components. The host machine needs outbound access to pull Docker images and dependencies during setup.
====

=== Containerized Testing Environment

End-to-end tests run in a containerized environment provided by the `integration-testing` module:

1. **NiFi Instance**: Running on HTTPS port 9095 with the MultiIssuerJWTTokenAuthenticator processor
   * Authentication with SingleUserLoginIdentityProvider
   * Credentials: admin/adminadminadmin
   * Processor mounted via volume for easy updates during development

2. **Keycloak Server**: Running on HTTP port 9080 and HTTPS port 9085
   * Admin credentials: admin/admin
   * Pre-configured realm (`oauth_integration_tests`) with:
     * Test user: testUser/drowssap
     * Test client: test_client/yTKslWLtf4giJcWCaoVJ20H8sy6STexM

3. **Certificate Configuration**:
   * Self-signed certificate for localhost (1 year validity)
   * NiFi: PKCS12 format (keystore.p12, truststore.p12)
   * Keycloak: PEM format (localhost.crt, localhost.key)

4. **Browser Environment**: Cross-browser testing with Cypress supports:
   * Chrome (latest and latest-1 versions)
   * Firefox (latest and latest-1 versions)
   * Edge (latest version)
   * Safari (latest version, for MacOS test environments only)

   The primary development and testing browser is Chrome, with automated cross-browser testing implemented in CI/CD pipelines.

=== Starting the Test Environment

To start the test environment:

[source,bash]
----
# From the project root
./integration-testing/src/main/docker/run-test-container.sh
----

This script:
1. Builds the processor NAR file
2. Checks certificates
3. Starts the NiFi and Keycloak containers
4. Waits for the services to be healthy

To stop the environment:

[source,bash]
----
./integration-testing/src/main/docker/stop-test-container.sh
----

[WARNING]
====
The test environment uses self-signed certificates and predefined credentials that are intended for testing purposes only. Never use these credentials or certificates in production environments.
====

=== Test Data Management

Test data is managed through:

1. **Predefined Configurations**: Standard processor configurations for different test scenarios
2. **Token Generation**: Real JWT tokens from the Keycloak instance
3. **JWKS Endpoints**: Real JWKS endpoints from the Keycloak instance
4. **Test Users and Roles**: Predefined users with different permissions in the Keycloak realm

=== Obtaining Test Tokens

To obtain a test token from Keycloak:

[source,bash]
----
curl -X POST \
  http://localhost:9080/realms/oauth_integration_tests/protocol/openid-connect/token \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -d 'grant_type=password&client_id=test_client&client_secret=yTKslWLtf4giJcWCaoVJ20H8sy6STexM&username=testUser&password=drowssap'
----

This returns a JSON response containing an `access_token` that can be used for testing.

=== JWKS Endpoints

The Keycloak instance provides real JWKS endpoints:

* HTTP: `http://keycloak:9080/realms/oauth_integration_tests/protocol/openid-connect/certs`
* HTTPS: `https://keycloak:9085/realms/oauth_integration_tests/protocol/openid-connect/certs`

These endpoints can be used to configure the processor for testing.

== End-to-End Test Scenarios

=== Processor Configuration Flow

==== Basic Configuration Test

Tests the basic configuration flow:

1. Navigate to NiFi canvas
2. Add MultiIssuerJWTTokenAuthenticator processor if not present
3. Configure basic properties (token location, header name)
4. Configure advanced properties (token size, refresh interval)
5. Save configuration
6. Verify configuration is persisted correctly

==== Issuer Configuration Test

Tests the issuer configuration flow:

1. Navigate to processor configuration
2. Right-click on the processor and select "Advanced"
3. Add a new issuer with JWKS-Server type
4. Enter JWKS URL and validate connection
5. Configure audience, scopes, and roles
6. Save issuer configuration
7. Verify issuer is added to the processor configuration
8. Repeat for Local File and In Memory issuer types

==== Configuration Validation Test

Tests configuration validation:

1. Enter invalid values for properties
2. Verify appropriate validation errors are displayed
3. Enter valid values
4. Verify validation passes
5. Test required vs. optional fields

=== Token Verification Flow

==== Valid Token Verification Test

Tests the token verification flow with valid tokens:

1. Navigate to the Verification tab
2. Paste a valid JWT token
3. Click Verify Token
4. Verify token details are displayed correctly
5. Verify claims are parsed and displayed
6. Test tokens from different issuers

==== Invalid Token Verification Test

Tests the token verification flow with invalid tokens:

1. Test with expired token
2. Test with token from unknown issuer
3. Test with token having invalid signature
4. Test with token missing required claims
5. Test with malformed token
6. Verify appropriate error messages are displayed

=== JWKS Validation Flow

==== JWKS Server Validation Test

Tests the JWKS server validation flow:

1. Enter valid JWKS server URL
2. Click Validate button
3. Verify successful validation message
4. Test with invalid URL
5. Test with URL returning invalid JWKS
6. Test with URL returning error status
7. Verify appropriate error messages are displayed

==== Local File Validation Test

Tests the local file validation flow:

1. Enter valid file path
2. Click Validate button
3. Verify successful validation message
4. Test with non-existent file
5. Test with file containing invalid JWKS
6. Verify appropriate error messages are displayed

==== In Memory Validation Test

Tests the in-memory JWKS validation flow:

1. Paste valid JWKS content
2. Click Validate button
3. Verify successful validation message
4. Test with invalid JWKS content
5. Verify appropriate error messages are displayed

=== Metrics and Statistics Flow

Tests the metrics and statistics display:

1. Process flow files with valid and invalid tokens
2. Navigate to Metrics tab
3. Verify metrics are updated correctly
4. Verify statistics reflect actual processing results
5. Test metrics reset functionality

=== Internationalization Flow

Tests the internationalization support:

1. Change browser language setting
2. Verify UI elements are displayed in the correct language
3. Test with different languages (English, German, etc.)
4. Verify error messages are translated correctly

=== Accessibility Testing Flow

Tests the accessibility compliance of the UI:

1. **Keyboard Navigation**: Verify all UI components can be navigated using only the keyboard
2. **Screen Reader Compatibility**: Test with screen readers to ensure content is properly announced
3. **Color Contrast**: Verify UI meets WCAG 2.1 AA contrast requirements
4. **Form Labels**: Ensure all form elements have proper labels and ARIA attributes
5. **Focus Management**: Verify focus handling in modals and dynamic content
6. **Responsive Design**: Test UI functionality at different zoom levels

[NOTE]
====
Accessibility testing uses automated tools like axe-core integrated with Cypress, plus manual verification with screen readers such as NVDA or VoiceOver.
====

== Test Implementation

=== Test Code Structure

The end-to-end tests are implemented using Cypress for UI testing with the following directory structure:

[source]
----
nifi-cuioss-ui/
├── cypress/
│   ├── fixtures/              # Test data
│   │   ├── tokens/            # JWT tokens for testing
│   │   └── jwks/              # JWKS files for testing
│   ├── integration/           # Test specifications
│   │   ├── configuration/     # Processor configuration tests
│   │   ├── verification/      # Token verification tests
│   │   └── metrics/           # Metrics display tests
│   ├── plugins/               # Cypress plugins
│   ├── support/               # Support code
│   │   ├── page-objects/      # Page object classes
│   │   ├── commands.js        # Custom Cypress commands
│   │   └── console-warnings-allowlist.js  # Allowed console warnings
│   └── screenshots/           # Test failure screenshots
├── scripts/                   # Utility scripts
│   ├── generate-test-tokens.js  # Token generation utility
│   └── analyze-console-errors.js  # Console error analysis
└── package.json               # NPM configuration
----

These tests are organized by feature area and test specific user interactions with the UI.

=== Cypress UI Tests

Cypress tests form the foundation of our end-to-end testing strategy, focusing on UI interactions and user flows. We use data-testid attributes for more reliable selectors and implement page object patterns for better test maintainability.

==== Test Structure and Organization

Tests are organized by feature area in the following structure:

[source,javascript]
----
// Page Objects - Reusable UI interaction patterns
cypress/support/page-objects/
  ├── processor-configuration.js  // Methods for configuring processors
  ├── token-verification.js       // Methods for token verification workflows
  └── nifi-canvas.js              // Methods for NiFi canvas navigation

// Custom Commands - Shared functionality across tests
cypress/support/commands.js       // Includes login, navigation helpers

// Tests organized by feature
cypress/integration/
  ├── configuration/              // Processor configuration tests
  ├── verification/               // Token verification tests
  └── metrics/                    // Metrics display tests
----

==== Test Implementation

A typical Cypress test follows this pattern:

[source,javascript]
----
describe('Basic Processor Configuration', () => {
  beforeEach(() => {
    // Login to NiFi and navigate to canvas
    cy.login('admin', 'adminadminadmin');
    cy.visit('https://localhost:9095/nifi/');
    cy.get('[data-testid="flow-status-container"]', { timeout: 10000 }).should('be.visible');
  });

  it('should configure processor with Keycloak JWKS endpoint', () => {
    // Add processor to canvas
    cy.get('[data-testid="component-toolbar"] [data-testid="add-processor-button"]').click();
    cy.get('[data-testid="processor-type-filter"]').type('MultiIssuerJWTTokenAuthenticator');
    cy.get('[data-testid="processor-type-item"]:contains("MultiIssuerJWTTokenAuthenticator")').click();
    cy.get('[data-testid="processor-config-ok-button"]').click();

    // Open processor configuration
    cy.get('[data-testid="processor-component"]').rightclick();
    cy.get('[data-testid="context-menu-item"]:contains("Configure")').click();

    // Configure basic properties
    cy.get('[data-testid="property-input"][name="jwt.validation.token.location"]').select('AUTHORIZATION_HEADER');
    cy.get('[data-testid="property-input"][name="jwt.validation.token.header"]').clear().type('Authorization');

    // Add Keycloak issuer
    cy.get('[data-testid="dynamic-property-add-button"]').click();
    cy.get('[data-testid="dynamic-property-name"]').type('keycloak');
    cy.get('[data-testid="dynamic-property-value"]').type('http://keycloak:9080/realms/oauth_integration_tests/protocol/openid-connect/certs');

    // Validate JWKS endpoint
    cy.get('[data-testid="verify-jwks-button"]').click();
    cy.get('[data-testid="verification-result"]', { timeout: 5000 }).should('contain', 'Connection successful');

    // Save configuration
    cy.get('[data-testid="processor-config-ok-button"]').click();
  });
});
----

==== Page Object Pattern Implementation

To improve maintainability, we implement the Page Object pattern:

[source,javascript]
----
// cypress/support/page-objects/processor-configuration.js
class ProcessorConfigurationPage {
  // Selectors
  getPropertyInput(name) {
    return cy.get(`[data-testid="property-input"][name="${name}"]`);
  }

  getDynamicPropertyAddButton() {
    return cy.get('[data-testid="dynamic-property-add-button"]');
  }

  getDynamicPropertyNameInput() {
    return cy.get('[data-testid="dynamic-property-name"]');
  }

  getDynamicPropertyValueInput() {
    return cy.get('[data-testid="dynamic-property-value"]');
  }

  getVerifyJwksButton() {
    return cy.get('[data-testid="verify-jwks-button"]');
  }

  getVerificationResult() {
    return cy.get('[data-testid="verification-result"]');
  }

  getOkButton() {
    return cy.get('[data-testid="processor-config-ok-button"]');
  }

  // Actions
  setBasicProperties() {
    this.getPropertyInput('jwt.validation.token.location').select('AUTHORIZATION_HEADER');
    this.getPropertyInput('jwt.validation.token.header').clear().type('Authorization');
    return this;
  }

  addIssuer(name, url) {
    this.getDynamicPropertyAddButton().click();
    this.getDynamicPropertyNameInput().type(name);
    this.getDynamicPropertyValueInput().type(url);
    return this;
  }

  validateJwksEndpoint() {
    this.getVerifyJwksButton().click();
    this.getVerificationResult().should('contain', 'Connection successful');
    return this;
  }

  saveConfiguration() {
    this.getOkButton().click();
  }
}

export default new ProcessorConfigurationPage();
----

==== Using Page Objects in Tests

With page objects, the tests become more readable and maintainable:

[source,javascript]
----
// cypress/integration/configuration/basic-configuration.spec.js
import ProcessorConfigurationPage from '../../support/page-objects/processor-configuration';
import NifiCanvasPage from '../../support/page-objects/nifi-canvas';

describe('Basic Processor Configuration', () => {
  beforeEach(() => {
    cy.login('admin', 'adminadminadmin');
    cy.visit('https://localhost:9095/nifi/');
    NifiCanvasPage.waitForCanvasToLoad();
  });

  it('should configure processor with Keycloak JWKS endpoint', () => {
    // Add processor to canvas
    NifiCanvasPage.addProcessor('MultiIssuerJWTTokenAuthenticator');

    // Open processor configuration
    NifiCanvasPage.openProcessorConfiguration();

    // Configure processor
    ProcessorConfigurationPage
      .setBasicProperties()
      .addIssuer('keycloak', 'http://keycloak:9080/realms/oauth_integration_tests/protocol/openid-connect/certs')
      .validateJwksEndpoint()
      .saveConfiguration();

    // Verify processor is properly configured
    NifiCanvasPage.assertProcessorIsValid();
  });
});
----

==== Custom Commands

We extend Cypress with custom commands for common operations:

[source,javascript]
----
// cypress/support/commands.js
Cypress.Commands.add('login', (username, password) => {
  cy.session([username, password], () => {
    cy.visit('https://localhost:9095/nifi/');
    cy.get('[data-testid="username-input"]').type(username);
    cy.get('[data-testid="password-input"]').type(password);
    cy.get('[data-testid="login-button"]').click();
    cy.get('[data-testid="flow-status-container"]', { timeout: 15000 }).should('be.visible');
  });
});

Cypress.Commands.add('navigateToProcessorVerification', () => {
  cy.get('[data-testid="processor-component"]').rightclick();
  cy.get('[data-testid="context-menu-item"]:contains("Verification")').click();
});
----

==== Handling Asynchronous Operations

NiFi operations can be asynchronous. We implement robust waiting strategies:

[source,javascript]
----
// Handling asynchronous operations
it('should verify token processing results', () => {
  // Submit token for processing
  cy.get('[data-testid="process-token-button"]').click();

  // Wait for processing to complete with configurable timeout
  cy.get('[data-testid="processing-status"]', { timeout: 30000 })
    .should('have.text', 'Completed');

  // Use retry-ability for potentially unstable assertions
  cy.get('[data-testid="token-attributes"]')
    .should('contain', 'jwt.content.sub')
    .should('contain', 'testUser');
});
----

==== Cross-Browser Testing

Our tests are designed to run across multiple browsers with appropriate handling for browser-specific behaviors:

[source,javascript]
----
// Browser-specific handling
it('should handle file uploads across browsers', () => {
  // Different browsers have different file upload mechanisms
  if (Cypress.isBrowser('firefox')) {
    cy.get('[data-testid="file-input"]').selectFile('cypress/fixtures/jwks/keycloak-jwks.json', { force: true });
  } else {
    cy.get('[data-testid="file-input"]').selectFile('cypress/fixtures/jwks/keycloak-jwks.json');
  }

  // Common validation
  cy.get('[data-testid="file-name"]').should('contain', 'keycloak-jwks.json');
});
----

==== Visual Testing

For UI components, we implement visual testing:

[source,javascript]
----
// Visual validation of UI components
it('should display token claims correctly', () => {
  // Load token and navigate to verification screen
  cy.fixture('tokens/valid-tokens.json').then(({ validToken }) => {
    cy.navigateToProcessorVerification();
    cy.get('[data-testid="token-input"]').type(validToken);
    cy.get('[data-testid="verify-token-button"]').click();

    // Check that claims table is displayed correctly
    cy.get('[data-testid="claims-table"]').should('be.visible');

    // Take screenshot for visual comparison
    cy.get('[data-testid="claims-container"]').screenshot('token-claims-display');
  });
});
----

==== Console Error Verification

We consistently verify that no unexpected console errors or warnings occur during test execution. This is crucial for ensuring a clean implementation and identifying potential issues that might be hidden from the UI:

[source,javascript]
----
// Console error verification implementation
describe('Console Error Checking', () => {
  // Track console errors and warnings
  const consoleErrors = [];
  const consoleWarnings = [];
  const allowedWarnings = [
    // Define a positive list of allowed warnings
    'Warning: validateDOMNesting(...): <div> cannot appear as a descendant of <p>.',
    'DevTools failed to load source map',
    'Content Security Policy violation for inline script'
  ];

  beforeEach(() => {
    // Clear previous errors/warnings
    consoleErrors.length = 0;
    consoleWarnings.length = 0;

    // Intercept console.error
    cy.window().then((win) => {
      cy.stub(win.console, 'error').callsFake((msg) => {
        consoleErrors.push(msg);
      });

      // Intercept console.warn
      cy.stub(win.console, 'warn').callsFake((msg) => {
        // Only track warnings that are not in the allowed list
        if (!allowedWarnings.some(allowed => msg.includes(allowed))) {
          consoleWarnings.push(msg);
        }
      });
    });
  });

  afterEach(() => {
    // Verify no unexpected console errors
    expect(consoleErrors.length).to.equal(0, 
      `Found ${consoleErrors.length} console errors: ${consoleErrors.join(', ')}`);

    // Verify no unexpected console warnings
    expect(consoleWarnings.length).to.equal(0, 
      `Found ${consoleWarnings.length} console warnings: ${consoleWarnings.join(', ')}`);
  });

  it('processor configuration should not produce console errors', () => {
    // Test configuration flow
    cy.login('admin', 'adminadminadmin');
    cy.visit('https://localhost:9095/nifi/');
    // ...test implementation...
  });
});
----

The allowed warnings list is maintained as a centralized, documented exception list to:

1. **Prevent Test Noise**: Ignore known third-party library warnings that cannot be fixed
2. **Focus on Real Issues**: Ensure actual application errors are caught and addressed
3. **Document Technical Debt**: Clearly document known issues that are accepted

The list of allowed warnings should be reviewed periodically, and items should be removed when the underlying issues are fixed.

For a complete implementation example with centralized allowlist and reusable commands, see the link:examples/console-error-checking.js[Console Error Checking Example].

This comprehensive approach to Cypress testing enables us to thoroughly test the MultiIssuerJWTTokenAuthenticator processor's UI in a real environment, ensuring all user flows work correctly and that the browser console remains free of unexpected errors.

=== End-to-End Test Workflow

A complete end-to-end test with Cypress typically follows this workflow:

1. **Setup**: Login to NiFi and navigate to the canvas
2. **Processor Creation**: Add the MultiIssuerJWTTokenAuthenticator processor to the canvas
3. **Basic Configuration**: Configure token location, header name, etc.
4. **Issuer Configuration**: Add and configure issuers with different types (JWKS server, local file, in-memory)
5. **Validation**: Verify JWKS connections and validate configuration
6. **Test Operation**: Test token verification with different token types
7. **Verification**: Assert that the UI displays expected results

This workflow tests the entire user experience from processor setup to token verification, ensuring all UI components work together correctly.

== Test Data Generation

=== Cypress Test Fixtures

Cypress tests use fixtures to provide test data. These fixtures are stored in the `cypress/fixtures` directory and include token examples and configuration data:

[source,javascript]
----
// cypress/fixtures/tokens/valid-tokens.json
{
  "validToken": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...",
  "adminToken": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...",
  "expectedSubject": "testUser",
  "expectedIssuer": "http://localhost:9080/realms/oauth_integration_tests"
}

// cypress/fixtures/tokens/invalid-tokens.json
{
  "expiredToken": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...",
  "invalidSignatureToken": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9..."
}

// cypress/fixtures/jwks/keycloak-jwks.json
{
  "keys": [
    {
      "kid": "YvGl1VhRlUe-Cf_9k3X6K2MI8JyFo5V0mGCK5U1QlXA",
      "kty": "RSA",
      "alg": "RS256",
      "use": "sig",
      "n": "pPr5h-b9VBQDI...",
      "e": "AQAB"
    }
  ]
}
----

These fixtures can be loaded in Cypress tests:

[source,javascript]
----
describe('Token Verification', () => {
  let validTokens, invalidTokens;

  before(() => {
    // Load test data
    cy.fixture('tokens/valid-tokens.json').then(data => {
      validTokens = data;
    });
    cy.fixture('tokens/invalid-tokens.json').then(data => {
      invalidTokens = data;
    });
  });

  it('should verify a valid token', () => {
    // Navigate to verification UI
    cy.visit('https://localhost:9095/nifi/');
    cy.navigateToProcessorVerification();

    // Paste token and verify
    cy.get('[data-testid="token-input"]').type(validTokens.validToken);
    cy.get('[data-testid="verify-token-button"]').click();

    // Check results
    cy.get('[data-testid="token-subject"]').should('contain', validTokens.expectedSubject);
    cy.get('[data-testid="token-issuer"]').should('contain', validTokens.expectedIssuer);
  });
});
----

=== Token Generation Helper

To generate real tokens for testing, we use a utility script that obtains tokens from the Keycloak instance:

[source,javascript]
----
// cypress/support/token-generator.js
const axios = require('axios');
const fs = require('fs');
const path = require('path');

/**
 * Utility for obtaining real tokens from Keycloak for testing
 */
class TokenGenerator {
  constructor() {
    this.keycloakUrl = 'http://localhost:9080';
    this.realm = 'oauth_integration_tests';
    this.clientId = 'test_client';
    this.clientSecret = 'yTKslWLtf4giJcWCaoVJ20H8sy6STexM';
    this.username = 'testUser';
    this.password = 'drowssap';
  }

  /**
   * Get a valid token from Keycloak
   */
  async getValidToken() {
    return this.getToken(this.username, this.password);
  }

  /**
   * Get a token with custom scopes
   */
  async getTokenWithScopes(scopes) {
    return this.getToken(this.username, this.password, scopes.join(' '));
  }

  /**
   * Get a token for a specific user
   */
  async getToken(username, password, scope = null) {
    try {
      // Build the token request
      const params = new URLSearchParams();
      params.append('grant_type', 'password');
      params.append('client_id', this.clientId);
      params.append('client_secret', this.clientSecret);
      params.append('username', username);
      params.append('password', password);

      if (scope) {
        params.append('scope', scope);
      }

      // Send request
      const response = await axios.post(
        `${this.keycloakUrl}/realms/${this.realm}/protocol/openid-connect/token`,
        params,
        {
          headers: {
            'Content-Type': 'application/x-www-form-urlencoded'
          }
        }
      );

      // Return access token
      return response.data.access_token;
    } catch (error) {
      console.error('Failed to get token from Keycloak', error);
      throw error;
    }
  }

  /**
   * Save tokens to fixture files for Cypress tests
   */
  async saveTokensToFixtures() {
    // Get tokens
    const validToken = await this.getValidToken();
    const adminToken = await this.getTokenWithScopes(['admin']);

    // Create fixtures directory if it doesn't exist
    const fixturesDir = path.join(__dirname, '..', 'fixtures', 'tokens');
    if (!fs.existsSync(fixturesDir)) {
      fs.mkdirSync(fixturesDir, { recursive: true });
    }

    // Save valid token fixture
    fs.writeFileSync(
      path.join(fixturesDir, 'valid-tokens.json'),
      JSON.stringify({
        validToken,
        adminToken,
        expectedSubject: this.username,
        expectedIssuer: `${this.keycloakUrl}/realms/${this.realm}`
      }, null, 2)
    );

    // For invalid tokens, we can tamper with valid tokens
    // In a real implementation, you'd need to implement token tampering
    const expiredToken = validToken; // Replace with actual expired token
    const invalidSignatureToken = validToken.slice(0, -5) + 'XXXXX'; // Simple tampering

    // Save invalid token fixture
    fs.writeFileSync(
      path.join(fixturesDir, 'invalid-tokens.json'),
      JSON.stringify({
        expiredToken,
        invalidSignatureToken
      }, null, 2)
    );

    console.log('Token fixtures saved successfully');
  }
}

module.exports = new TokenGenerator();
----

This generator can be run as a pre-test script to generate fresh tokens:

[source,javascript]
----
// scripts/generate-test-tokens.js
const tokenGenerator = require('../cypress/support/token-generator');

(async () => {
  try {
    await tokenGenerator.saveTokensToFixtures();
    console.log('Test tokens generated successfully');
  } catch (error) {
    console.error('Error generating test tokens:', error);
    process.exit(1);
  }
})();
----

=== JWKS Endpoints for Cypress

The Keycloak instance provides real JWKS endpoints that can be used in Cypress tests:

[source,javascript]
----
// cypress/support/jwks-endpoints.js
/**
 * Utility for working with real JWKS endpoints from Keycloak
 */
class JwksEndpoints {
  /**
   * Get the HTTP JWKS endpoint URL for local access
   */
  getLocalHttpJwksUrl() {
    return 'http://localhost:9080/realms/oauth_integration_tests/protocol/openid-connect/certs';
  }

  /**
   * Get the HTTPS JWKS endpoint URL for local access
   */
  getLocalHttpsJwksUrl() {
    return 'https://localhost:9085/realms/oauth_integration_tests/protocol/openid-connect/certs';
  }

  /**
   * Get the HTTP JWKS endpoint URL for Docker container access
   */
  getContainerHttpJwksUrl() {
    return 'http://keycloak:9080/realms/oauth_integration_tests/protocol/openid-connect/certs';
  }

  /**
   * Get the HTTPS JWKS endpoint URL for Docker container access
   */
  getContainerHttpsJwksUrl() {
    return 'https://keycloak:9085/realms/oauth_integration_tests/protocol/openid-connect/certs';
  }
}

module.exports = new JwksEndpoints();
----

== Test Execution

=== Local Execution

To run Cypress end-to-end tests locally:

1. Start the integration-testing environment:
+
[source,bash]
----
# From the project root
./integration-testing/src/main/docker/run-test-container.sh
----

2. Generate fresh test tokens (optional):
+
[source,bash]
----
# Generate fresh test tokens
cd nifi-cuioss-ui
npm run generate-tokens
----

3. Run the Cypress tests:
+
[source,bash]
----
# Run Cypress tests in headless mode
cd nifi-cuioss-ui
npm run e2e:test

# Run Cypress tests in interactive mode
npm run e2e:open
----

4. View test results:
+
[source,bash]
----
# Cypress test results
open nifi-cuioss-ui/cypress/reports/index.html
----

5. Stop the test environment:
+
[source,bash]
----
./integration-testing/src/main/docker/stop-test-container.sh
----

=== Debugging Cypress Tests

For debugging Cypress tests:

1. Run tests in interactive mode:
+
[source,bash]
----
cd nifi-cuioss-ui
npm run e2e:open
----

2. Use Cypress debugging features:
   * Use the Cypress Test Runner to inspect elements
   * Add `.debug()` to pause test execution
   * Use the browser's developer tools during test execution
   * View screenshots and videos of test runs in the `cypress/screenshots` and `cypress/videos` directories

3. Add debug logging in tests:
+
[source,javascript]
----
// Add debug logging
it('should verify a token', () => {
  cy.log('Starting token verification test');

  // Get token from fixture
  cy.fixture('tokens/valid-tokens.json').then(fixtures => {
    cy.log(`Using token with subject: ${fixtures.expectedSubject}`);

    // Test continues...
  });
});
----

4. View logs from the NiFi container:
+
[source,bash]
----
# View application log
docker compose -f integration-testing/src/main/docker/docker-compose.yml logs nifi

# Follow logs
docker compose -f integration-testing/src/main/docker/docker-compose.yml exec nifi tail -f /opt/nifi/nifi-current/logs/nifi-app.log
----

5. View logs from the Keycloak container:
+
[source,bash]
----
docker compose -f integration-testing/src/main/docker/docker-compose.yml logs keycloak
----

=== CI/CD Integration

Cypress tests are integrated into the CI/CD pipeline:

1. The integration-testing environment is started automatically in CI
2. Cypress tests run in headless mode with video recording enabled
3. Test results are published as GitHub artifacts
4. Test failures block merges to protected branches

The CI workflow includes these steps:

[source,yaml]
----
jobs:
  cypress-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: 'npm'
          cache-dependency-path: nifi-cuioss-ui/package-lock.json

      - name: Start integration testing environment
        run: ./integration-testing/src/main/docker/run-test-container.sh

      - name: Install dependencies
        run: |
          cd nifi-cuioss-ui
          npm ci

      - name: Generate test tokens
        run: |
          cd nifi-cuioss-ui
          npm run generate-tokens

      - name: Run Cypress tests
        run: |
          cd nifi-cuioss-ui
          npm run e2e:test

      - name: Analyze console errors
        run: |
          cd nifi-cuioss-ui
          node scripts/analyze-console-errors.js $(date +%Y%m%d%H%M%S)

      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: cypress-results
          path: |
            nifi-cuioss-ui/cypress/reports/
            nifi-cuioss-ui/cypress/videos/
            nifi-cuioss-ui/cypress/screenshots/
            nifi-cuioss-ui/cypress/reports/console-analysis/
----

== Test Results Analysis

After running Cypress end-to-end tests, it's important to analyze the results effectively:

=== Cypress Test Report Structure

Cypress test reports are generated in the following location:
* **Cypress Reports**: `nifi-cuioss-ui/cypress/reports/`

The reports include the following key information:
* Test execution times
* Failure details with stack traces
* Screenshots for UI test failures
* Video recordings of test runs

Cypress generates detailed HTML reports that can be viewed in a browser, with features like:
* Test run statistics and summaries
* Failure details with contextual information
* Timeline view of test execution
* Environment details

=== Common Cypress Test Failure Patterns

When analyzing Cypress test failures, look for these common patterns:

1. **Element Interaction Issues**:
   * Element not found errors (`cy.get() failed because the element could not be found`)
   * Element not visible or not clickable errors
   * Element state issues (e.g., disabled elements, elements in the wrong state)

2. **Timing Issues**:
   * Actions performed before page is ready
   * Assertions running before UI has updated
   * Network requests not completing in time

3. **Authentication Problems**:
   * Login failures
   * Session expiration
   * Token issues

4. **UI Validation Failures**:
   * Expected text or values not appearing
   * Incorrect form validation behavior
   * UI not updating as expected after actions

5. **Console Error Failures**:
   * Unexpected console errors appearing during test execution
   * Console warnings not in the allowed warnings list
   * Transient console errors that appear only under specific conditions

=== Interpreting Cypress Test Results

When evaluating Cypress test results, consider the following:

1. **Test Stability**: Are failures consistent or intermittent?
2. **Visual Evidence**: Review screenshots and videos to understand the UI state
3. **Error Messages**: Analyze error messages and stack traces for clues
4. **Test Environment**: Check if failures are environment-specific
5. **Browser Compatibility**: Determine if failures are browser-specific

To determine if a failure is a flaky test or a real issue:
1. Rerun the failing test in isolation using `npm run e2e:test -- --spec "cypress/integration/path/to/spec.js"`
2. Check if the failure is reproducible in different browsers
3. Examine network logs and response times
4. Review application logs for related errors

=== Console Error Analysis

When tests fail due to console errors or warnings, follow this analysis process:

1. **Categorize the Errors**:
   * **Application Errors**: Issues in your application code
   * **Framework Errors**: Issues related to React, Angular, or other frameworks
   * **Third-Party Library Errors**: Issues from external dependencies
   * **Network Errors**: Failed API calls or resource loading issues

2. **Determine Severity**:
   * **Critical**: Affects core functionality or security (always fix)
   * **Major**: Affects important features (prioritize fixing)
   * **Minor**: Affects non-critical features (schedule for later)
   * **Cosmetic**: Does not affect functionality (consider for allowed list)

3. **Analyze Root Cause**:
   * Examine the error stack trace to identify source location
   * Check the test step that triggered the error
   * Verify if the error is reproducible outside of tests
   * Determine if it's browser-specific

4. **Decision Process for Allowed Warnings**:
   * Can the issue be fixed in our code? → Fix immediately
   * Is it from a third-party library we maintain? → Update the library
   * Is it from an external dependency we can't modify? → Consider for allowed list
   * Is it a known framework limitation? → Document and add to allowed list

5. **Documentation Requirements**:
   * For each allowed warning, document:
     * Exact warning pattern
     * Source of the warning
     * Reason it can't be fixed
     * Impact assessment
     * Future mitigation plan
     * Review date

The following tool helps generate console error reports from test runs:

[source,javascript]
----
// scripts/analyze-console-errors.js
const fs = require('fs');
const path = require('path');
const allowedWarnings = require('../cypress/support/console-warnings-allowlist');

// Parse Cypress console logs from test runs
function analyzeConsoleErrors(runId) {
  const logPath = path.join(__dirname, '..', 'cypress', 'logs', `run-${runId}.json`);
  const logs = JSON.parse(fs.readFileSync(logPath, 'utf8'));

  const errors = [];
  const unexpectedWarnings = [];
  const allowedWarningInstances = [];

  logs.forEach(log => {
    if (log.type === 'error') {
      errors.push({
        message: log.message,
        source: log.source,
        timestamp: log.timestamp,
        testFile: log.testFile,
        testName: log.testName
      });
    } else if (log.type === 'warning') {
      const isAllowed = allowedWarnings.some(pattern => 
        log.message.includes(pattern)
      );

      if (isAllowed) {
        allowedWarningInstances.push({
          message: log.message,
          pattern: allowedWarnings.find(pattern => log.message.includes(pattern)),
          source: log.source,
          testFile: log.testFile
        });
      } else {
        unexpectedWarnings.push({
          message: log.message,
          source: log.source,
          timestamp: log.timestamp,
          testFile: log.testFile,
          testName: log.testName
        });
      }
    }
  });

  // Generate report
  const report = {
    summary: {
      totalErrors: errors.length,
      totalUnexpectedWarnings: unexpectedWarnings.length,
      totalAllowedWarnings: allowedWarningInstances.length
    },
    errors,
    unexpectedWarnings,
    allowedWarningInstancesByPattern: groupByPattern(allowedWarningInstances)
  };

  // Write report
  const reportPath = path.join(__dirname, '..', 'cypress', 'reports', 'console-analysis', `run-${runId}.json`);
  fs.mkdirSync(path.dirname(reportPath), { recursive: true });
  fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));

  console.log(`Console error analysis complete. Report saved to ${reportPath}`);
  return report;
}

// Group allowed warnings by pattern for easier analysis
function groupByPattern(allowedWarnings) {
  const grouped = {};

  allowedWarnings.forEach(warning => {
    if (!grouped[warning.pattern]) {
      grouped[warning.pattern] = [];
    }
    grouped[warning.pattern].push(warning);
  });

  return grouped;
}

// Example usage
if (require.main === module) {
  const runId = process.argv[2];
  if (!runId) {
    console.error('Please provide a run ID');
    process.exit(1);
  }

  const report = analyzeConsoleErrors(runId);
  console.log(`Found ${report.summary.totalErrors} errors and ${report.summary.totalUnexpectedWarnings} unexpected warnings`);
}

module.exports = { analyzeConsoleErrors };
----

This analysis provides insights into console errors and helps maintain the allowed warnings list over time.

For a comprehensive implementation of the console error analysis tool, see the link:examples/analyze-console-errors.js[Console Error Analysis Script].

== Test Maintenance

=== Best Practices

1. **Keep Tests Independent**: Each test should be self-contained
2. **Use Page Objects**: Abstract UI interactions into reusable components
3. **Minimize Flakiness**: Use proper waiting and assertions
4. **Maintain Test Data**: Keep test data up-to-date with application changes
5. **Document Test Scenarios**: Each test should have clear documentation
6. **Verify Console Output**: Ensure no unexpected errors or warnings appear in the browser console

=== Console Error Management

The browser console is an important indicator of application quality. Our tests actively monitor and verify console output:

==== Allowed Warnings Policy

We maintain a centralized "allowed warnings" list in the `console-warnings-allowlist.js` file:

[source,javascript]
----
// cypress/support/console-warnings-allowlist.js
module.exports = [
  // Third-party library warnings that cannot be fixed
  'Warning: validateDOMNesting(...): <div> cannot appear as a descendant of <p>.',
  'DevTools failed to load source map',
  'Content Security Policy violation for inline script',

  // Deprecated API usage warnings from third-party libraries
  'Synchronous XMLHttpRequest on the main thread is deprecated',

  // Browser-specific warnings
  '[Firefox] Unable to preventdefault inside passive event listener',
  '[Chrome] Provider for: vscode-resource'
];
----

==== Management Process

1. **All Console Errors Fail Tests**: By default, any console error causes test failure
2. **Limited Warning Allowlist**: Only documented, unavoidable warnings are allowed
3. **Regular Reviews**: The allowed warnings list is reviewed quarterly
4. **Clear Documentation**: Each allowed warning must have a documented justification
5. **Root Cause Resolution**: Where possible, address warnings rather than allowing them

==== Adding to the Allowlist

To add a warning to the allowed list:

1. Create a ticket documenting the warning
2. Investigate the root cause
3. Determine if it can be fixed in our code
4. If unfixable, document justification
5. Add to the allowlist with a comment explaining why it cannot be fixed
6. Schedule periodic review date

This process ensures we maintain high-quality code with minimal technical debt.

For an example of package.json scripts that support console error validation, see the link:examples/package-with-console-checks.json[Package with Console Checks].

=== Troubleshooting

Common issues and solutions:

1. **Flaky Tests**: 
   * If tests are inconsistent, add more explicit waits and retry logic
   * Use Cypress's built-in retry capabilities for assertions
   * Consider using `cy.waitUntil()` for complex conditions
   * Add logging to identify timing issues

2. **Selector Changes**: 
   * If UI selectors change, update page objects in a single place
   * Use data-testid attributes in the UI for more stable selectors
   * Consider using more specific selectors to avoid accidental matches

3. **Test Data Issues**: 
   * If test data becomes invalid, regenerate using the provided utilities
   * Create test data immediately before use to ensure freshness
   * Use unique identifiers for test entities to prevent collision

4. **Environment Problems**: 
   * If the test environment fails to start, check Docker logs
   * Verify network connectivity between containers
   * Ensure sufficient system resources are available
   * Check certificate validity and trust issues

5. **Authentication Issues**: 
   * If login fails, verify Keycloak configuration and credentials
   * Check token expiration settings
   * Validate that JWKS endpoints are accessible
   * Monitor HTTP response codes for auth-related failures

=== Test Data Cleanup

After test execution, it's important to clean up test data to maintain a consistent environment:

1. Reset Keycloak realm to initial state using the provided scripts
2. Clean up any test data created in NiFi
3. Remove generated test tokens and JWKS files
4. Reset metrics and counters in the processor

For automated cleanup in CI environments, use the provided cleanup script:

[source,bash]
----
./integration-testing/src/main/docker/cleanup-test-environment.sh
----

== Implementation Roadmap

=== Phase 1: Setup and Infrastructure

1. Set up Cypress and required dependencies
2. Create Docker-based test environment
3. Implement basic test utilities and helpers
4. Create test data generation scripts

=== Phase 2: Basic Test Implementation

1. Implement processor configuration tests
2. Implement token verification tests
3. Implement JWKS validation tests
4. Add CI/CD integration
5. Implement console error/warning verification with allowed warnings list

=== Phase 3: Advanced Test Implementation

1. Implement metrics and statistics tests
2. Implement internationalization tests
3. Enhance cross-browser testing with Cypress
4. Implement performance and load testing

=== Phase 4: Maintenance and Expansion

1. Create documentation and training materials
2. Implement monitoring for test reliability
3. Expand test coverage to edge cases
4. Integrate with overall quality metrics

== Conclusion

End-to-end testing is a critical component of ensuring the MultiIssuerJWTTokenAuthenticator processor functions correctly from a user perspective. By implementing the testing strategy outlined in this document, we can:

1. Verify that all UI components work correctly together
2. Ensure that user flows function as expected
3. Detect regression issues early in the development process
4. Provide confidence in the quality of the processor

The implementation of this end-to-end testing plan will significantly improve the reliability and user experience of the MultiIssuerJWTTokenAuthenticator processor.

== See Also

* link:testing.adoc[Testing]
* link:../../e-2-e-cypress/doc/javascript-testing-guide.md[JavaScript Testing Implementation Guide]
* link:configuration-ui.adoc[UI Configuration]
* link:token-validation.adoc[Token Validation]
* link:../Requirements.adoc#NIFI-AUTH-16[Testing Requirements]
* link:../library/cui-test-keycloak-integration/README.adoc[Keycloak Integration Testing]
* link:../integration-testing/README.adoc[Integration Testing Environment]
* link:../Specification.adoc[Back to Main Specification]

== Code Quality Standards

The Cypress end-to-end testing framework follows the centralized JavaScript and ESLint standards defined in the organization's coding standards repository. 

[NOTE]
====
For complete ESLint configuration details, rule explanations, and implementation guidelines, see the centralized standards at `/standards/javascript/cypress-e2e-testing-standards.adoc` and `/standards/javascript/linting-standards.adoc`.
====

=== Local Implementation

This project implements the centralized standards with:

* **Zero-warning ESLint configuration** achieving 98 warnings → 0 warnings
* **Production-ready `.eslintrc.js`** with Cypress-optimized rules
* **Constants-based architecture** for duplicate string management
* **Maven integration** with validation in the build lifecycle
* **Comprehensive file-specific overrides** for different test scenarios

See the working configuration in `.eslintrc.js` and `cypress/support/constants.js` for the successful implementation of the centralized standards.
