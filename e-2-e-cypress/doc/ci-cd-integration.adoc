= CI/CD Integration
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

== Purpose

This document covers continuous integration and deployment setup for the NiFi integration testing framework.

== GitHub Actions Workflow

=== Main Workflow Configuration

The project uses GitHub Actions for automated testing and deployment:

[source,yaml]
----
# .github/workflows/e2e-tests.yml
name: End-to-End Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

jobs:
  e2e-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        node-version: [20.x]
        java-version: [11, 17]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          java-version: ${{ matrix.java-version }}
          distribution: 'temurin'
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: e-2-e-cypress/package-lock.json
          
      - name: Cache Maven dependencies
        uses: actions/cache@v3
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2
          
      - name: Start test environment
        run: |
          cd integration-testing
          docker-compose up -d
          ./src/main/docker/check-status.sh
          
      - name: Build and test
        run: |
          mvn clean install -Pui-tests
          
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.java-version }}
          path: |
            e-2-e-cypress/cypress/screenshots/
            e-2-e-cypress/cypress/videos/
            e-2-e-cypress/tests-report/
            
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./e-2-e-cypress/coverage/lcov.info
          flags: e2e-tests
          name: e2e-coverage
----

=== Pull Request Workflow

[source,yaml]
----
# .github/workflows/pr-validation.yml
name: Pull Request Validation

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  validate-changes:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'
          
      - name: Run affected tests only
        run: |
          # Detect changed files
          CHANGED_FILES=$(git diff --name-only origin/main...HEAD)
          
          if echo "$CHANGED_FILES" | grep -q "e-2-e-cypress/"; then
            echo "E2E tests affected, running full suite"
            mvn clean install -Pui-tests
          else
            echo "E2E tests not affected, running selftests only"
            mvn clean install -Pselftests
          fi
----

== Environment Variables

The CI/CD process uses these environment variables:

* `CYPRESS_BASE_URL`: https://localhost:8443/nifi
* `CYPRESS_KEYCLOAK_URL`: https://localhost:8443/auth

== Artifacts Collected

=== Test Results (30-day retention)

* `e-2-e-cypress/tests-report/` - JUnit XML and HTML reports
* `e-2-e-cypress/cypress/videos/` - Test execution videos
* `e-2-e-cypress/cypress/screenshots/` - Failure screenshots

=== Frontend Test Results (7-day retention)

* `nifi-cuioss-ui/coverage/` - Code coverage reports
* `e-2-e-cypress/tests-report/` - Self-test results

=== Test Reports (for GitHub integration)

* `e-2-e-cypress/tests-report/*.xml` - JUnit XML for test reporting

== Console Error Analysis

On test failures, the workflow automatically:

1. Searches test reports for console errors
2. Runs console error analysis script if available
3. Reports findings in the workflow output

Manual console error analysis:

[source,bash]
----
# Analyze console errors for a specific run
npm run analyze:console [run-id]

# Example
npm run analyze:console 2025-06-11T14-30-00
----

== Working with CI/CD

=== Local Development

Before pushing changes:

[source,bash]
----
# Run quality checks locally
npm run lint:check
npm run format:check

# Run self-tests (fast unit tests)
npm run test:failfast

# Run full E2E tests (requires test environment)
cd ../integration-testing
./run-test-container.sh
cd ../e-2-e-cypress
npm run cypress:run
----

=== Debugging CI Failures

1. Check workflow logs in GitHub Actions tab
2. Download artifacts for detailed analysis
3. Review console error analysis if available
4. Run locally to reproduce issues

=== Artifact Download

To download and analyze test artifacts:

1. Go to the failed workflow run in GitHub
2. Scroll to "Artifacts" section
3. Download relevant artifact zip files
4. Extract and review:
   * HTML reports for visual test results
   * Videos to see what happened during tests
   * Screenshots of failures
   * JUnit XML for detailed test data

== Performance Considerations

=== Caching Strategy

* *Maven dependencies*: Cached by POM file hash
* *NPM dependencies*: Cached by package-lock.json hash
* *Node.js binaries*: Cached by setup-node action

=== Parallel Execution

* Frontend quality checks and E2E tests run sequentially
* This prevents resource conflicts and ensures quality gates

=== Timeout Configuration

* Service readiness: 5 minutes (300 seconds)
* Individual test timeouts: Configured in Cypress settings
* Overall job timeout: GitHub Actions default (6 hours)

== Maintenance

=== Regular Tasks

1. Monitor test stability - Check for flaky tests
2. Update dependencies - Keep workflow actions current
3. Review artifacts retention - Adjust as needed
4. Analyze console errors - Regular review of allowed warnings

=== Updating the Workflow

When modifying `.github/workflows/e2e-tests.yml`:

1. Test changes in a feature branch
2. Verify all jobs complete successfully
3. Check artifact collection works
4. Update documentation if needed

== Troubleshooting

=== Common Issues

*Docker service startup failures*:

* Check Docker daemon status
* Verify sufficient system resources
* Review container logs in workflow output

*Environment readiness timeouts*:

* Services may take longer to start in CI
* Check service health check endpoints
* Verify port configurations

*Artifact upload failures*:

* Check path specifications in workflow
* Ensure artifacts exist before upload
* Verify retention settings

*Console error analysis missing*:

* Ensure console logging is configured in Cypress
* Check that log files are generated
* Verify script permissions and syntax

=== Getting Help

1. Check workflow logs for specific error messages
2. Review this documentation for common solutions
3. Examine successful runs for comparison
4. Test locally to isolate CI-specific issues

== Security Considerations

* No sensitive data in workflow logs
* Environment variables properly scoped
* Artifacts automatically expire
* Workflow permissions follow least-privilege principle

== Maven Integration

=== Profile Configuration

The Maven build integrates testing profiles for different scenarios:

[source,xml]
----
<!-- pom.xml profiles -->
<profiles>
  <profile>
    <id>selftests</id>
    <activation>
      <activeByDefault>true</activeByDefault>
    </activation>
    <properties>
      <cypress.spec>cypress/e2e/selftests/**/*.cy.js</cypress.spec>
      <docker.skip>true</docker.skip>
    </properties>
  </profile>
  
  <profile>
    <id>ui-tests</id>
    <properties>
      <cypress.spec>cypress/e2e/**/*.cy.js</cypress.spec>
      <docker.skip>false</docker.skip>
      <cypress.auto.start>true</cypress.auto.start>
    </properties>
    <build>
      <plugins>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-failsafe-plugin</artifactId>
          <executions>
            <execution>
              <id>start-containers</id>
              <phase>pre-integration-test</phase>
              <goals>
                <goal>integration-test</goal>
              </goals>
              <configuration>
                <includes>
                  <include>**/ContainerStartupIT.java</include>
                </includes>
              </configuration>
            </execution>
          </executions>
        </plugin>
      </plugins>
    </build>
  </profile>
</profiles>
----

=== Build Lifecycle Integration

[source,xml]
----
<!-- Frontend Maven Plugin for Node.js/Cypress -->
<plugin>
  <groupId>com.github.eirslett</groupId>
  <artifactId>frontend-maven-plugin</artifactId>
  <version>1.15.0</version>
  <configuration>
    <nodeVersion>v20.11.0</nodeVersion>
    <npmVersion>10.4.0</npmVersion>
    <workingDirectory>e-2-e-cypress</workingDirectory>
  </configuration>
  <executions>
    <execution>
      <id>install-node-and-npm</id>
      <phase>initialize</phase>
      <goals>
        <goal>install-node-and-npm</goal>
      </goals>
    </execution>
    <execution>
      <id>npm-install</id>
      <phase>initialize</phase>
      <goals>
        <goal>npm</goal>
      </goals>
      <configuration>
        <arguments>ci</arguments>
      </configuration>
    </execution>
    <execution>
      <id>cypress-tests</id>
      <phase>integration-test</phase>
      <goals>
        <goal>npm</goal>
      </goals>
      <configuration>
        <arguments>run test:ci</arguments>
        <skip>${skipTests}</skip>
      </configuration>
    </execution>
  </executions>
</plugin>
----

== Quality Gates

=== ESLint Integration

Following centralized JavaScript standards:

[source,yaml]
----
# Quality check step in CI
- name: Run ESLint
  run: |
    cd e-2-e-cypress
    npm run lint
    
    # Ensure zero warnings
    WARNINGS=$(npm run lint 2>&1 | grep -c "warning" || true)
    if [ "$WARNINGS" -gt 0 ]; then
      echo "❌ ESLint warnings found: $WARNINGS"
      exit 1
    fi
    
    echo "✅ ESLint passed with zero warnings"
----

=== Coverage Thresholds

[source,javascript]
----
// jest.config.js (for JavaScript components)
module.exports = {
  collectCoverageFrom: [
    'src/main/javascript/**/*.js',
    '!src/main/javascript/vendor/**'
  ],
  coverageThreshold: {
    global: {
      branches: 90,
      functions: 90,
      lines: 90,
      statements: 90
    }
  },
  coverageReporters: ['text', 'lcov', 'html', 'json-summary']
};
----

=== Performance Budgets

[source,yaml]
----
# Performance monitoring in CI
- name: Performance Budget Check
  run: |
    cd e-2-e-cypress
    
    # Extract performance metrics from test results
    AVG_TEST_TIME=$(jq '.runs[].stats.duration' tests-report/combined-report.json | jq -s 'add/length')
    
    # Check against budget (60 seconds)
    if (( $(echo "$AVG_TEST_TIME > 60000" | bc -l) )); then
      echo "❌ Performance budget exceeded: ${AVG_TEST_TIME}ms > 60000ms"
      exit 1
    fi
    
    echo "✅ Performance budget met: ${AVG_TEST_TIME}ms"
----

== Test Reporting

=== Cypress Dashboard Integration

[source,javascript]
----
// cypress.config.js
module.exports = {
  projectId: 'nifi-extensions-e2e',
  e2e: {
    setupNodeEvents(on, config) {
      // Test result processing
      on('after:run', (results) => {
        if (results) {
          const stats = {
            totalTests: results.totalTests,
            totalPassed: results.totalPassed,
            totalFailed: results.totalFailed,
            totalSkipped: results.totalSkipped,
            duration: results.totalDuration
          };
          
          console.log('Test Statistics:', JSON.stringify(stats, null, 2));
          
          // Send to CI dashboard
          if (process.env.CI) {
            sendToDashboard(stats);
          }
        }
      });
    }
  }
};
----

== Local Development Integration

=== Pre-commit Hooks

[source,bash]
----
#!/bin/bash
# .git/hooks/pre-commit

echo "Running pre-commit checks..."

# ESLint check
cd e-2-e-cypress
if ! npm run lint; then
  echo "❌ ESLint failed. Fix errors before committing."
  exit 1
fi

# Quick selftests
if ! npm run test:failfast; then
  echo "❌ Selftests failed. Fix errors before committing."
  exit 1
fi

echo "✅ Pre-commit checks passed"
----

=== Development Workflow

[source,bash]
----
#!/bin/bash
# Development workflow

# Complete development workflow
echo "🚀 Starting development workflow..."

# 1. Code quality check
echo "1️⃣ Running code quality checks..."
cd e-2-e-cypress
npm run lint:fix
npm run format:check

# 2. Run tests
echo "2️⃣ Running tests..."
npm run test:failfast

# 3. Start containers for integration tests
echo "3️⃣ Starting test environment..."
cd ../integration-testing
docker-compose up -d
./src/main/docker/check-status.sh

# 4. Full integration tests
echo "4️⃣ Running full test suite..."
cd ../e-2-e-cypress
npm run cypress:run

echo "✅ Development workflow completed successfully!"
----

== See Also

* xref:setup-guide.adoc[Setup Guide] - Environment setup instructions
* xref:testing-patterns.adoc[Testing Patterns] - Implementation patterns and examples
* xref:overview.adoc[Project Overview] - High-level project description
